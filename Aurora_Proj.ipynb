{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a915d-eb61-4a25-81e6-cce2e8a1e385",
   "metadata": {},
   "source": [
    "Image Segmentation\n",
    "\n",
    "Using multiple models to complete the task and find the best one.\n",
    "- Mask R-CNN\n",
    "- Transformer\n",
    "- UNet\n",
    "- Transformer + UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e1109-2daf-4ec8-b094-4dd2e2b97490",
   "metadata": {},
   "source": [
    "1.Obtain a dataset and explain the problem you are trying to solve. This will characterise the type of predictive model you can build\n",
    "  \n",
    "2.Explore the data to gain insights.\n",
    "  Visualize and explain the main trends in the data briefly â€“ use visualisation to explain the problem concisely.\n",
    "  \n",
    "3.Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    "\n",
    "4.Explore different models and shortlist the best ones. This might be as simple as training multiple different networks on the same problem.\n",
    "\n",
    "5.Fine-tune your models. What is considered good performance will depend on the problem you tackle.\n",
    "\n",
    "6.Present your final solution with any summary conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33c689d-adc7-4668-8be5-618f12485c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "############################\n",
    "Set Up\n",
    "############################\n",
    "'''\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"UCL-BA/Predictive Analysis/Individual Proj\"\n",
    "IMG_FOLDER = \"figs\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", IMG_FOLDER)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c5f266-01b9-4000-b16a-9f16b0625afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "device = torch.device('cuda')\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False,figsize=(12, 12))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4b642-1f93-4476-ac91-34bf16edd3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab63ba-03ca-4235-8589-50856b43bde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96349f42-49ef-485e-a335-7ba9ec652b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abcfe8-df1b-4798-ae6b-d96b0fe7fd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de3e61-88c0-44d0-a0fe-b07e4cd117a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd7aa1-723f-4baa-b9e5-364b310b7b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213967c-bb02-4e7c-aadb-d1e92e08dd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c26087-af1a-4fda-9805-4d3d7b9a3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1162fa-4c6e-408c-97b7-ed148f455b3e",
   "metadata": {},
   "source": [
    "References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2e2d3-e73f-469d-a458-80b50dd53fc7",
   "metadata": {},
   "source": [
    "Bandyopadhyay, H. (no date) Image segmentation: Deep Learning vs traditional [guide], Image Segmentation: Deep Learning vs Traditional [Guide]. Available at: https://www.v7labs.com/blog/image-segmentation-guide (Accessed: March 14, 2023). \n",
    "\n",
    "From NLP to CV (no date) Transformers: from NLP to CV. Available at: https://ibrahimsobh.github.io/Transformers/ (Accessed: March 14, 2023). \n",
    "\n",
    "Hardebro, M. and Jirskog, E. (2022) Transformer based object detection and semantic segmentation for autonomous driving, DIVA. Available at: https://liu.diva-portal.org/smash/record.jsf?pid=diva2%3A1678704 (Accessed: March 14, 2023). \n",
    "\n",
    "Mwiti, D. (2023) Image segmentation: Architectures, losses, datasets, and Frameworks, neptune.ai. Available at: https://neptune.ai/blog/image-segmentation (Accessed: March 14, 2023). \n",
    "\n",
    "Yin, Z. (2022) Real-World Image Restoration based on Attention Mechanism. dissertation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
